ci.test(as.numeric(y1), as.numeric(u2)) # y(1) and y(0) depend on u2
ci.test(as.numeric(y0), as.numeric(u2))
# EY(1)
mean(y1)
# EY(1) using the adjustment for C
mean(y[a==1 & c==1])* mean(c) + mean(y[a==1 & c==0])*(1-mean(c))
# EY(0)
mean(y0)
# EY(0) using the adjustment for C
mean(y[a==0 & c==1])* mean(c) + mean(y[a==0 & c==0])*(1-mean(c))
# Estimation -------------------------------------------------------------
# ATE
mean(y1) - mean(y0)
# ATE estimated by adjusting Y on C
lm(y ~ a + c)
mean(y[a==1 & c==1])* mean(c) + mean(y[a==1 & c==0])*(1-mean(c)) -
mean(y[a==0 & c==1])* mean(c) - mean(y[a==0 & c==0])*(1-mean(c))
# EY(1)
mean(y1)
# EY(1) using the adjustment for C
mean(y[a==1 & c==1])* mean(c) + mean(y[a==1 & c==0])*(1-mean(c))
# EY(0)
mean(y0)
# EY(0) using the adjustment for C
mean(y[a==0 & c==1])* mean(c) + mean(y[a==0 & c==0])*(1-mean(c))
# Author: Juha Karvanen/code: Tetiana Gorbach
# Date: 2024-05-17
# This code presents an example of a DAg with a trpdoor variable
# Load required library
library(mvtnorm)
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(292377111)
# Set the sample size
n <- 10000000
# Define confounders c1, c2
errors <- rmvnorm(n, sigma = diag(4))
# Generate confounders
u1 <- errors[, 1]
u2 <- errors[, 2]
c1 <- rbinom(n, size = 1, prob = plogis(3 + u1 + u2)) # 3 + u1 + u2 +   errors[, 1]
c2 <- rbinom(n, size = 1, prob = plogis(1 + c1)) # 1 +  c1 + errors[, 2]
# Generate binary treatment that depends on c2
a <- rbinom(n, size = 1, prob = plogis(c2 + u1))
# Generate potential outcomes
y1 <- 4 + u2 + errors[, 3]
y0 <- 2 + u2 + errors[, 4]
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# EY(1)
mean(y1)
# EY(1) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
mean(y0)
# EY(0) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 0 & c2 == 1]))* (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 1 & c2 == 1]))* mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
# Check ignorability ------------------------------------------------------
ci.test(y0, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y0 is dependent on a given c1, c2: weak unconfoundedness is fulfilled
ci.test(y1, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y1 is dependent on a given c1, c2: weak unconfoundedness is fulfilled
# Estimation -------------------------------------------------------------
# Average treatment effect is equal to
mean(y1) -mean(y0)
# Avergae treatment effect estimated by adjusting y for c1 and c2
lm(y ~ a + c1 + c2)
rm(list = ls())
set.seed(292377111)
# Set the sample size
n <- 1000000
v <- rbinom(n , size = 1, p = 0.5)
u <- rbinom(n , size = 1, p = 0.5)
c1 <- rbinom(n, size = 1, prob = 0.4*u + 0.4*v)
c2 <- rbinom(n, size = 1, prob = 0.4 + 0.4*c1)
a <- rbinom(n, size = 1, prob = 0.4*c2 + 0.4*v)
y <- rbinom(n, size = 1, prob = 0.4*a + 0.4*u)
# EY(1) = P(Y=1|do(X)=1)
(0.2 + 0.4)^1*(0.8 - 0.4)^(1-1)
# EY(1) estimate using the identification and c2=1
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
# EY(1) estimate using the identification and c2=0
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
(0.2 + 0.4*0)^1*(0.8 - 0.4*0)^(1-1)
# EY(0) estimate using the identification and c2 =1
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1- mean(a[c1 == 1 & c2 == 1])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
# EY(0) estimate using the identification and c2 =0
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
# Estimation -------------------------------------------------------------
# Average treatment effect is equal to
mean(y1) -mean(y0)
# EY(1)
mean(y1)
# Author: Juha Karvanen/code: Tetiana Gorbach
# Date: 2024-05-17
# This code presents an example of a DAG with a trapdoor variable
# Load required libraries
library(mvtnorm)
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(292377111)
# Set the sample size
n <- 10000000
# Define confounders c1, c2
errors <- rmvnorm(n, sigma = diag(4))
# Generate confounders
u1 <- errors[, 1]
u2 <- errors[, 2]
c1 <- rbinom(n, size = 1, prob = plogis(3 + u1 + u2)) # 3 + u1 + u2 +   errors[, 1]
c2 <- rbinom(n, size = 1, prob = plogis(1 + c1)) # 1 +  c1 + errors[, 2]
# Generate binary treatment that depends on c2 and u1
a <- rbinom(n, size = 1, prob = plogis(c2 + u1))
# Generate potential outcomes that depend on u2
y1 <- 4 + u2 + errors[, 3]
y0 <- 2 + u2 + errors[, 4]
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# EY(1)
mean(y1)
# EY(1) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
mean(y0)
# EY(0) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 0 & c2 == 1]))* (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 1 & c2 == 1]))* mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
# Check ignorability ------------------------------------------------------
ci.test(y0, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y0 is dependent on a given c1, c2: weak unconfoundedness is fulfilled
# Generate data for a model with a cycle that includes
# a binary treatment, two confounders with a cyclic relationship, continuous outcome
# Graph is defined by paths: c1 -> a, c2 -> treat, c1 -> c2, c2 -> c1, c1 -> Y, c2 -> Y, treat -> Y
# Load required libraries
library(mvtnorm) # to simulate multivariate normal
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(123)
# Set the sample size
n <- 1000000
# Define confounders c1, c2 with a cyclic relationship
errors <- rmvnorm(n, sigma = diag(4))
error1 <- errors[, 1]
error2 <- errors[, 2]
# c1 = 0.1*c2 + error1
# c2 = 0.1*c1 + error2
# in  a matrix form c = c*B + e. Then c = e*(I-B)^(-1)
B <- matrix(c(0, 0.1, 0.1, 0), nrow = 2, byrow = T)
inverse_i_b <- solve(diag(2) - B) # (I-B)^-1
confouders <- cbind(error1, error2) %*% inverse_i_b # n*2 matrix of confounders c1, c2
c1 <- confouders[, 1]
c2 <- confouders[, 2]
# Generate binary treatment that depends on c2
a <- rbinom(n, size = 1, prob = plogis(c1 + c2))
# Generate potential outcomes
y1 <- 4 + c1 + c2 + errors[, 3]
y0 <- 2 + c1 + c2 + errors[, 4]
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# Average treatment effect is equal to Ey1 - Ey0 = 4 +  Ec1 + Ec2 - (2 + Ec1 + Ec2) = 4 - 2 = 2
mean(y1) - mean(y0)
# Estimate DAGs from the observed and potential outcomes data -------------
# Estimate a graph from the observed data
df <- as.data.frame(cbind(a, c1, c2, y))
# Estimate the Completed Partially Directed Acyclic Graph (CPDAG)
cpdag <- bnlearn::pc.stable(df)
# Plot the CPDAG to visualize the inferred causal relationships
plot(cpdag)
# Test for conditional independence of y and a given c2
ci.test(y, as.numeric(a), data.frame(c2)) # y depends on a given  c2
# Drawing a graph from the unobserved potential outcomes and the treatment variable
# but y1 is independent of a given c2: weak unconfoundness is fulfilled
# Estimate a graph from the potential outcomes data
df2 <- as.data.frame(cbind(a, c1, c2, y1))
cpdag2 <- bnlearn::pc.stable(df2)
# Plot the graph
plot(cpdag2)
# Check ignorability  -----------------------------------------------------
ci.test(y0, as.numeric(a), data.frame(c1, c2))
ci.test(y1, as.numeric(a), data.frame(c1, c2)) # y1 is independent of a given c1, c2: weak unconfoundedness is fulfilled
# Author: Juha Karvanen/code: Tetiana Gorbach
# Date: 2024-05-17
# This code presents an example of a DAG with a trapdoor variable
# Load required libraries
library(mvtnorm)
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(292377111)
# Set the sample size
n <- 10000000
# Define confounders c1, c2
errors <- rmvnorm(n, sigma = diag(4))
# Generate confounders
u1 <- errors[, 1]
u2 <- errors[, 2]
c1 <- rbinom(n, size = 1, prob = plogis(3 + u1 + u2)) # 3 + u1 + u2 +   errors[, 1]
c2 <- rbinom(n, size = 1, prob = plogis(1 + c1)) # 1 +  c1 + errors[, 2]
# Generate binary treatment that depends on c2 and u1
a <- rbinom(n, size = 1, prob = plogis(c2 + u1))
# Generate potential outcomes that depend on u2
y1 <- 4 + u2 + errors[, 3]
y0 <- 2 + u2 + errors[, 4]
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# EY(1)
mean(y1)
# EY(1) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
mean(y0)
# EY(0) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 0 & c2 == 1]))* (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 1 & c2 == 1]))* mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
# Check ignorability ------------------------------------------------------
ci.test(y0, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y0 is dependent on a given c1, c2: weak unconfoundedness is not fulfilled
ci.test(y1, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y1 is dependent on a given c1, c2: weak unconfoundedness is not fulfilled
# Estimation -------------------------------------------------------------
# Average treatment effect is equal to
mean(y1) -  mean(y0)
# Average treatment effect estimated by adjusting y for c1 and c2
lm(y ~ a + c1 + c2)
# Author: Juha Karvanen/code: Tetiana Gorbach
# Date: 2024-05-17
# This code presents an example of a DAG with a trapdoor variable
# Load required libraries
library(mvtnorm)
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(292377111)
# Set the sample size
n <- 100000
# Define confounders c1, c2
errors <- rmvnorm(n, sigma = diag(4))
# Generate confounders
u1 <- errors[, 1]
u2 <- errors[, 2]
c1 <- rbinom(n, size = 1, prob = plogis(3 + u1 + u2)) # 3 + u1 + u2 +   errors[, 1]
c2 <- rbinom(n, size = 1, prob = plogis(1 + c1)) # 1 +  c1 + errors[, 2]
# Generate binary treatment that depends on c2 and u1
a <- rbinom(n, size = 1, prob = plogis(c2 + u1))
# Generate potential outcomes that depend on u2
y1 <- 4 + u2 + errors[, 3]
y0 <- 2 + u2 + errors[, 4]
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# EY(1)
mean(y1)
# EY(1) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
mean(y0)
# EY(0) estimate using the identification
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 0 & c2 == 1]))* (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1 - mean(a[c1 == 1 & c2 == 1]))* mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
# Check ignorability ------------------------------------------------------
ci.test(y0, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y0 is dependent on a given c1, c2: weak unconfoundedness is not fulfilled
ci.test(y1, as.numeric(a), data.frame(as.numeric(c1), as.numeric(c2))) # y1 is dependent on a given c1, c2: weak unconfoundedness is not fulfilled
# Estimation -------------------------------------------------------------
# Average treatment effect is equal to
mean(y1) -  mean(y0)
# Average treatment effect estimated by adjusting y for c1 and c2
lm(y ~ a + c1 + c2)
# Helske's example --------------------------------------------------------
rm(list = ls())
set.seed(292377111)
# Set the sample size
n <- 1000000
v <- rbinom(n , size = 1, p = 0.5)
u <- rbinom(n , size = 1, p = 0.5)
c1 <- rbinom(n, size = 1, prob = 0.4*u + 0.4*v)
c2 <- rbinom(n, size = 1, prob = 0.4 + 0.4*c1)
a <- rbinom(n, size = 1, prob = 0.4*c2 + 0.4*v)
y <- rbinom(n, size = 1, prob = 0.4*a + 0.4*u)
# EY(1) = P(Y=1|do(X)=1)
(0.2 + 0.4)^1*(0.8 - 0.4)^(1-1)
# EY(1) estimate using the identification and c2=1
(mean(y[c1 == 0 & c2 == 1 & a == 1]) * mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 1]) * mean(a[c1 == 1 & c2 == 1]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 1]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 1]) * mean(c1))
# EY(1) estimate using the identification and c2=0
(mean(y[c1 == 0 & c2 == 0 & a == 1]) * mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 1]) * mean(a[c1 == 1 & c2 == 0]) * mean(c1)) /
(mean(a[c1 == 0 & c2 == 0]) * (1 - mean(c1)) + mean(a[c1 == 1 & c2 == 0]) * mean(c1))
# EY(0)
(0.2 + 0.4*0)^1*(0.8 - 0.4*0)^(1-1)
# EY(0) estimate using the identification and c2 =1
(mean(y[c1 == 0 & c2 == 1 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 1 & a == 0]) * (1- mean(a[c1 == 1 & c2 == 1])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 1])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 1])) * mean(c1))
# EY(0) estimate using the identification and c2 =0
(mean(y[c1 == 0 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) +
mean(y[c1 == 1 & c2 == 0 & a == 0]) * (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1)) /
((1-mean(a[c1 == 0 & c2 == 0])) * (1 - mean(c1)) + (1-mean(a[c1 == 1 & c2 == 0])) * mean(c1))
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
# Author: Juha Karvanen/Tetiana Gorbach
# Date: 2024-05-17
# This code presents an example of a complex front-door in Example 6.
# Load required libraries
library(mvtnorm)
library(bnlearn)
# Generate data -----------------------------------------------------------
# Set a seed for reproducibility
set.seed(292377111)
set.seed(0)
# Set the sample size
n <- 1000000
# errors <- rmvnorm(n, sigma = diag(8))
# # Generate confounders
# u_z_c1 <- errors[, 1]
# u_c1_c3 <- errors[, 2]
# u_a_c3 <- errors[, 3]
# u_a_ya <- errors[, 4]
u_z_c1 <- rbinom(n, size = 1, prob = 0.5)
u_c1_c3 <- rbinom(n, size = 1, prob = 0.5)
u_a_c3 <- rbinom(n, size = 1, prob = 0.5)
u_a_ya <- rbinom(n, size = 1, prob = 0.5)
c1 <- rbinom(n, size = 1, prob = plogis(u_z_c1 + u_c1_c3))
c2 <- rbinom(n, size = 1, prob = plogis(1 + c1))
c3 <- rbinom(n, size = 1, prob = plogis(1 + c2 + u_a_c3 + u_c1_c3))
# Generate a binary treatment
a <- rbinom(n, size = 1, prob = plogis(c3 + u_a_c3 + u_a_ya))
# Generate the mediator
z1 <- rbinom(n, size = 1, prob = plogis(1 + u_z_c1))
z0 <- rbinom(n, size = 1, prob = plogis(u_z_c1))
z <- ifelse(a == 1, z1, z0)
# Generate potential outcomes
y1 <- rbinom(n, size = 1, prob = plogis(z1 + u_a_ya))
y0 <- rbinom(n, size = 1, prob = plogis(z0 + u_a_ya))
# Generate observed outcome
y <- ifelse(a == 1, y1, y0)
# EY(1)
mean(y1)
# EY(0)
mean(y0)
# df2 <- as.data.frame(cbind(a=as.numeric(a), c1=as.numeric(c1), c2=as.numeric(c2),
#                            c3 = as.numeric(c3), z = as.numeric(z), y = as.numeric(y)))
# cpdag2 <- pc.stable(df2)
# # Plot the graph
# plot(cpdag2)
# Estimation --------------------------------------------------------------
pc1 <- function(c1_value) {
pc1_1 <- mean(c1)
pc1_1^c1_value * (1 - pc1_1)^(1 - c1_value)
}
pa_given_c1_c2 <- function(a_value, c1_value, c2_value) {
pa_1 <- mean(a[c1 == c1_value & c2 == c2_value])
pa_1^a_value * (1 - pa_1)^(1 - a_value)
}
pa_given_c1_c2_c3 <- function(a_value, c1_value, c2_value, c3_value) {
pa_1 <- mean(a[c1 == c1_value & c2 == c2_value & c3 == c3_value])
pa_1^a_value * (1 - pa_1)^(1 - a_value)
}
pc3_given_c1_c2 <- function(c3_value, c1_value, c2_value) {
pc3_1 <- mean(c3[c1 == c1_value & c2 == c2_value])
pc3_1^c3_value * (1 - pc3_1)^(1 - c3_value)
}
pz_given_c1_c2_a <- function(z_value, c1_value, c2_value, a_value) {
pz_1 <- mean(z[c1 == c1_value & c2 == c2_value &  a == a_value])
pz_1^z_value * (1 - pz_1)^(1 - z_value)
}
pz_given_c1_c2_c3_a <- function(z_value, c1_value, c2_value, c3_value, a_value) {
pz_1 <- mean(z[c1 == c1_value & c2 == c2_value & c3 == c3_value & a == a_value])
pz_1^z_value * (1 - pz_1)^(1 - z_value)
}
py_given_c1_c2_c3_a_z <- function(y_value, c1_value, c2_value, c3_value, a_value, z_value) {
py_1 <- mean(y[c1 == c1_value & c2 == c2_value & c3 == c3_value & a == a_value & z == z_value])
py_1^y_value * (1 - py_1)^(1 - y_value)
}
g1_a_c2 <- function(a_value, c2_value) {
pa_given_c1_c2(a_value = a_value, c1_value = 0, c2_value = c2_value) * pc1(0)  +
pa_given_c1_c2(a_value = a_value, c1_value = 1, c2_value = c2_value) * pc1(1)
}
g2_a_z_c2 <- function(a_value, z_value, c2_value) {
pz_given_c1_c2_a(z_value = z_value, c1_value = 0, c2_value = c2_value,
a_value = a_value) *
pa_given_c1_c2(a_value = a_value, c1_value = 0, c2_value = c2_value) *
pc1(0) +
pz_given_c1_c2_a(z_value = z_value, c1_value = 1, c2_value = c2_value,
a_value = a_value) *
pa_given_c1_c2(a_value = a_value, c1_value = 1, c2_value = c2_value) *
pc1(1)
}
g3_a_z_c2_c3 <- function(a_value, z_value, c2_value, c3_value) {
pz_given_c1_c2_c3_a(z_value = z_value, c1_value = 0, c2_value = c2_value,
c3_value = c3_value, a_value = a_value) *
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 0, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 0, c2_value = c2_value) *
pc1(0)  +
pz_given_c1_c2_c3_a(z_value = z_value, c1_value = 1, c2_value = c2_value,
c3_value = c3_value, a_value = a_value) *
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 1, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 1, c2_value = c2_value) *
pc1(1)
}
g4_a_c2_c3 <- function(a_value,  c2_value, c3_value) {
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 0, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 0, c2_value = c2_value) *
pc1(0)  +
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 1, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 1, c2_value = c2_value) *
pc1(1)
}
g5_y_a_z_c2_c3 <- function(y_value, a_value, z_value, c2_value, c3_value) {
py_given_c1_c2_c3_a_z(y_value = y_value, c1_value = 0,
c2_value = c2_value, c3_value = c3_value, a_value = a_value, z_value = z_value) *
pz_given_c1_c2_c3_a(z_value = z_value, c1_value = 0, c2_value = c2_value,
c3_value = c3_value, a_value = a_value) *
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 0, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 0, c2_value = c2_value) *
pc1(0)  +
py_given_c1_c2_c3_a_z(y_value = y_value, c1_value = 1,
c2_value = c2_value, c3_value = c3_value, a_value = a_value, z_value = z_value) *
pz_given_c1_c2_c3_a(z_value = z_value, c1_value = 1, c2_value = c2_value,
c3_value = c3_value, a_value = a_value ) *
pa_given_c1_c2_c3(a_value = a_value,
c1_value = 1, c2_value = c2_value, c3_value = c3_value) *
pc3_given_c1_c2(c3_value = c3_value, c1_value = 1, c2_value = c2_value) *
pc1(1)
}
# Initialize the sum
identification_sum <- function(a_value_def,c2_value_def, y_value_def ){
sum_outer <- 0
# Loop over all combinations of a, z and c3 in {0, 1}
for (z_value in c(0, 1)) {
for (c3_value in c(0, 1)){
sum_f <- 0
for (a_value in c(0, 1)) {
sum_f <- sum_f + g5_y_a_z_c2_c3(y_value = y_value_def,
a_value = a_value,
z_value = z_value,
c2_value = c2_value_def,
c3_value = c3_value)*
g4_a_c2_c3(a_value = a_value,
c2_value = c2_value_def,
c3_value = c3_value)/
g3_a_z_c2_c3(a_value = a_value,
z_value = z_value,
c2_value = c2_value_def,
c3_value = c3_value)
}
sum_outer <- sum_outer +
g2_a_z_c2(a_value = a_value_def,
z_value = z_value,
c2_value = c2_value_def) * sum_f
}
}
sum_outer/g1_a_c2(a_value = a_value_def, c2_value = c2_value_def)
}
identification_sum(a_value_def = 1, c2_value_def = 1, y_value_def = 1) # should be close to mean(y1)
identification_sum(a_value_def = 1, c2_value_def = 0, y_value_def = 1) # should be close to mean(y1)
mean(y1)
identification_sum(a_value_def = 0, c2_value_def = 1, y_value_def = 1) # should be close to mean(y0)
identification_sum(a_value_def = 0, c2_value_def = 0, y_value_def = 1) # should be close to mean(y0)
mean(y0)
styler:::style_selection()
